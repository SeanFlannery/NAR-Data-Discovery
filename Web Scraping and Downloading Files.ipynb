{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Downloading Files\n",
    "Author: Sean Flannery [sflanner@purdue.edu](sflanner@purdue.edu)\n",
    "\n",
    "Last Updated: June 13th, 2019\n",
    "\n",
    "This notebook was developed with the intent of satisfying data acquisition needs for work with\n",
    "Professor Daisuke Kihara [dkihara@purdue.edu](dkihara@purdue.edu).\n",
    "### Description\n",
    "*\"The first task is to download pdf files of all the papers from all the years. Each year, the location of pdf files may be a bit different.\"*\n",
    "\n",
    "We have provided a file `nardb.txt` that contains the respective **years** and **URLs** where we may find the annual collections of pertinent articles.\n",
    "\n",
    "**Libraries Needed:** \n",
    "[pandas](https://pandas.pydata.org/pandas-docs/stable/install.html), \n",
    "[numpy](https://www.numpy.org), \n",
    "[tqdm](https://github.com/tqdm/tqdm), \n",
    "[bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import urllib3\n",
    "from urllib.request import FancyURLopener\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data from local file describing years of article data to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://academic.oup.com/nar/issue/47/D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>https://academic.oup.com/nar/issue/46/D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>https://academic.oup.com/nar/issue/45/D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>https://academic.oup.com/nar/issue/44/D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>https://academic.oup.com/nar/issue/43/D1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                       url\n",
       "0  2019  https://academic.oup.com/nar/issue/47/D1\n",
       "1  2018  https://academic.oup.com/nar/issue/46/D1\n",
       "2  2017  https://academic.oup.com/nar/issue/45/D1\n",
       "3  2016  https://academic.oup.com/nar/issue/44/D1\n",
       "4  2015  https://academic.oup.com/nar/issue/43/D1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_df = pd.read_csv('nardb.txt', delim_whitespace=True, names = ['year','url'])\n",
    "year_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to actually navigate these websites now to discover the locations of the PDFs of interest (and possibly extract any useful data we might find).\n",
    "\n",
    "We shall use the `urllib` and `BeautifulSoup` libraries to first download and then analyze the given webpages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df['html-data'] = [None]*len(year_df)\n",
    "bs_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to grab a webppage from our dataset in `year_df` (this may take a bit of time).\n",
    "\n",
    "Note also that the error messages we check for are specific to the website we are crawling from (Nucleic Acids Research Database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabYearWebPage(index):\n",
    "    url = str(year_df.loc[index, 'url'])\n",
    "    content = requests.post(url).content\n",
    "    bs = BeautifulSoup(content, 'html.parser')\n",
    "    resStr = str(bs.prettify())\n",
    "    # NOTE: Hacky... These are just a couple of examples \n",
    "    if bs.find(id='captcha') is not None:\n",
    "        print(\"Captcha encountered!\")\n",
    "        exit() # We ought to stop and adjust something if this happens...\n",
    "        return None\n",
    "    if \"Your IP has been blacklisted due to excessive requests to the platform or suspicious activity.\" in resStr:\n",
    "        print(\"BLACKLISTED AT THIS IP!\")\n",
    "        exit()\n",
    "        return None\n",
    "    if \"Object reference not set to an instance of an object.\" in resStr:\n",
    "        print(\"Found error... Need to retry\")\n",
    "        exit()\n",
    "        return None\n",
    "    return {'bs': resStr, 'index': index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = year_df['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(url).content;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52622fb4677c44648eeb79ad127bfcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Pool(10) as p:\n",
    "        data_list = list(tqdm(p.imap(grabYearWebPage, range(len(year_df))), total=len(year_df), leave=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent MemoryErrors, we have to parse the requests outside of the concurrent mapping below. We then add them to our `year_df` under html-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataDict in data_list:\n",
    "    year_df.loc[dataDict['index'], 'html-data'] = dataDict['bs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "      <th>html-data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://academic.oup.com/nar/issue/47/D1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n &lt;head&gt;\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>https://academic.oup.com/nar/issue/46/D1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n &lt;head&gt;\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>https://academic.oup.com/nar/issue/45/D1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n &lt;head&gt;\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>https://academic.oup.com/nar/issue/44/D1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n &lt;head&gt;\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>https://academic.oup.com/nar/issue/43/D1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n &lt;head&gt;\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                       url  \\\n",
       "0  2019  https://academic.oup.com/nar/issue/47/D1   \n",
       "1  2018  https://academic.oup.com/nar/issue/46/D1   \n",
       "2  2017  https://academic.oup.com/nar/issue/45/D1   \n",
       "3  2016  https://academic.oup.com/nar/issue/44/D1   \n",
       "4  2015  https://academic.oup.com/nar/issue/43/D1   \n",
       "\n",
       "                                           html-data  \n",
       "0  <!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  ...  \n",
       "1  <!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  ...  \n",
       "2  <!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  ...  \n",
       "3  <!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  ...  \n",
       "4  <!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n  ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save acquired data into intermediate `year-YYYY-NAR.html` files in the `year_pages` directory for this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('year_pages'):\n",
    "    os.mkdir('year_pages')\n",
    "for urlID in range(len(year_df)):\n",
    "    fname = 'year_pages/year-' + str(year_df.loc[urlID, 'year']) + '-NAR.html'\n",
    "    file = open(fname, \"w+\")\n",
    "    file.write(year_df.loc[urlID, 'html-data'])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define some functions to encapsulate the process of grabbing the PDF article links from each site.\n",
    "\n",
    "Note that we import `re` to utilize some of python's regular expression tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabArticleLinks(index):\n",
    "    bs = BeautifulSoup(year_df['html-data'][index], 'html.parser')\n",
    "    # Filter through all citations on the page\n",
    "    filter_results = bs.findAll('div',  class_='ww-citation-primary')\n",
    "    htmlStr = ''.join(str(elem) for elem in filter_results)\n",
    "    # Grab the links from these sections for parsing again\n",
    "    bstmp = BeautifulSoup(htmlStr, 'html.parser')\n",
    "    page_links = bstmp.findAll('a', attrs={'href' : re.compile(\"http*\")})\n",
    "    # Create articledb entry for list of pages to crawl for pdfs\n",
    "    data = {'year':[year_df['year'][index]]*len(page_links), 'article-link':[]}\n",
    "    # Iterate over the links of the page\n",
    "    for link in page_links:\n",
    "        data['article-link'].append(link['href'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to parallelize the crawling process we defined above, as there can be substantial lag times waiting for the network to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7418acbd70f4b8c8451591352069629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Pool(20) as p:\n",
    "    resList = tqdm(p.map(grabArticleLinks, range(len(year_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to save all of this information as a CSV file for safety (and so we don't have to do all that crawling work once again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_dict = {'year':[], 'article-link':[]}\n",
    "for ent in resList:\n",
    "    final_dict['year'].extend(ent['year'])\n",
    "    final_dict['article-link'].extend(ent['article-link'])\n",
    "nar_df = pd.DataFrame.from_dict(final_dict)[['year', 'article-link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article-link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                         article-link\n",
       "0  2019  https://doi.org/10.1093/nar/gky1267\n",
       "1  2019   https://doi.org/10.1093/nar/gky993\n",
       "2  2019  https://doi.org/10.1093/nar/gky1124\n",
       "3  2019  https://doi.org/10.1093/nar/gky1069\n",
       "4  2019   https://doi.org/10.1093/nar/gky843"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to actually navigate these websites now to discover the locations of the PDFs of interest (and possibly extract any useful data we might find).\n",
    "\n",
    "We shall continue use the `urllib` and `BeautifulSoup` libraries to first download and then analyze the given webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some pre-processing to create the folders for our crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('articles'):\n",
    "    os.mkdir('articles')\n",
    "for year in set(nar_df['year']):\n",
    "    if not os.path.exists('articles/' + str(year)):\n",
    "        os.mkdir('articles/' + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to grab a webppage from our dataset in `nar_df` then store them all locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabAndSaveWebPage(urlID):\n",
    "    url = nar_df.loc[urlID, 'article-link']\n",
    "    content = None\n",
    "    while True:\n",
    "        try:\n",
    "            http = urllib3.PoolManager()\n",
    "            r = http.request('GET', url)\n",
    "            content = r.data\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Received Value Error... Continuing after url:\", str(url))\n",
    "            return 'ValueError'\n",
    "        except OSError:\n",
    "            print(\"Received OS Error... Continuing after url:\", str(url))\n",
    "            return 'OSError'\n",
    "        \n",
    "    bs = BeautifulSoup(content, 'html.parser')\n",
    "    resStr = str(bs.prettify())\n",
    "    if bs.find(id='captcha') is not None:\n",
    "        print(\"CAPTCHA\")\n",
    "        return 'CAPTCHA'\n",
    "    if \"Your IP has been blacklisted due to excessive requests to the platform or suspicious activity.\" in resStr:\n",
    "        print(\"BLACKLISTED AT THIS IP!\")\n",
    "        return 'BLACKLISTED'\n",
    "    if \"Object reference not set to an instance of an object.\" in resStr:\n",
    "        print(\"Found error... Need to retry\")\n",
    "        return 'WEIRD_ERROR'\n",
    "    fname = 'articles/' + str(nar_df[ 'year'][urlID]) + '/' + str(urlID) + '-NAR.html'\n",
    "    f = open(fname, \"w+\")\n",
    "    f.write(resStr)\n",
    "    f.flush()\n",
    "    f.close()\n",
    "    return 'SUCCESS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a useful function to get all URLs we have yet to download. We pass in our list of urlIDs we have just attempted to crawl, and return a list containing only entries that we have not completely downloaded (note that we also check file size since occasionally we were receiving files of size 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllUnscrapedUrlIDs(ids):\n",
    "    values = set(ids)\n",
    "    for root, dirs, files in os.walk(\"./articles\"):\n",
    "        # All files discovered so far\n",
    "        for filename in files:\n",
    "            # ignore hidden files based on machine or non-html\n",
    "            if filename[0] == '.' or 'NAR.html' not in filename:\n",
    "                continue\n",
    "            urlID = int(filename.replace(\"-NAR.html\", \"\"))\n",
    "            size = os.path.getsize('articles/' + str(nar_df['year'][urlID]) + '/' + filename)\n",
    "            # Non-Empty File Found! Don't want to search again\n",
    "            if size > 0:\n",
    "                if urlID in values:\n",
    "                    values.remove(urlID)\n",
    "    return list(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we do the actual scraping of the database. We will continuously attempt to get whatever webpages we couldn't recover before. The good thing is that you can stop running this for a bit and try again later or on a different connection to get more URLs... The behavior of the captcha protection can be inconsistent.\n",
    "\n",
    "In order to speed up our crawling, we will also import the multiprocessing packages of Python to enable simultaneous crawling. Initally we assume we want to get all of the urlIDs (we ensure we aren't doing extra work later using our prior method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = getAllUnscrapedUrlIDs(list(range(len(nar_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING: Prepare for a HOT CPU\n",
    "\n",
    "Also, feel free to adjust the number of processes spawned by Pool\n",
    "\n",
    "#### The captcha behavior is very inconsistent. \n",
    "You may end up with the same number of urlIDs not downloaded.\n",
    "Here are some things to try should that happen (with no reason in particular given as to why they might work).\n",
    "- Try pinging the `doi.org` server or `google.com` in a separate terminal window\n",
    "- Try connecting to a different network than you're on or use a VPN\n",
    "- Go to one of the sites that is not downloaded and complete the CAPTCHA on your current cconnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8488f92aea54714aa79f8d6f0c8f7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "{'WEIRD_ERROR'}\n",
      "Total Scraping Time over 11 undownloaded urls :{0.018} minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b95679061f4fbcbfa2efbc62b4f681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "{'WEIRD_ERROR'}\n",
      "Total Scraping Time over 11 undownloaded urls :{0.019} minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a57b985377417fa6e443d653df17fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "Found error... Need to retry\n",
      "{'WEIRD_ERROR'}\n",
      "Total Scraping Time over 11 undownloaded urls :{0.023} minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ee2f06797415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# random backoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetAllUnscrapedUrlIDs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ids = getAllUnscrapedUrlIDs(ids)\n",
    "while len(ids) != 0:\n",
    "    # random backoff \n",
    "    time.sleep(random.randint(1,3)*random.randint(1,3))\n",
    "    random.shuffle(getAllUnscrapedUrlIDs(ids))\n",
    "    start = time.time()\n",
    "    # Spawn processes to concurrently grab webpages\n",
    "    \n",
    "    with Pool(10) as p:\n",
    "        res_list = list(tqdm(p.imap(grabAndSaveWebPage, ids), total=len(ids), leave=True))\n",
    "    \n",
    "    print(set(res_list))\n",
    "    \n",
    "    print(\"Total Scraping Time over %d undownloaded urls :{%.3f} minutes\" % (len(ids),(time.time() - start)/60.))\n",
    "    ids = getAllUnscrapedUrlIDs(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have all of our needed files downloaded to the `articles` folder in the same directory as this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = getAllUnscrapedUrlIDs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs we failed to download (download manually)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article-link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, article-link]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"URLs we failed to download (download manually)\")\n",
    "nar_df.loc[ids, ['year', 'article-link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"articles\"):\n",
    "    # All files discovered so far\n",
    "    for filename in files:\n",
    "        # ignore hidden files based on machine or non-html\n",
    "        if filename[0] == '.' or filename is 'articles' or 'NAR.html' not in filename:\n",
    "            continue\n",
    "        urlID = int(filename.replace(\"-NAR.html\", \"\"))\n",
    "        nar_df.loc[urlID, 'local-path'] = 'articles/' + str(nar_df.loc[urlID,'year']) + '/' + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article-link</th>\n",
       "      <th>local-path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1267</td>\n",
       "      <td>articles/2019/0-NAR.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky993</td>\n",
       "      <td>articles/2019/1-NAR.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1124</td>\n",
       "      <td>articles/2019/2-NAR.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1069</td>\n",
       "      <td>articles/2019/3-NAR.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky843</td>\n",
       "      <td>articles/2019/4-NAR.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                         article-link                local-path\n",
       "0  2019  https://doi.org/10.1093/nar/gky1267  articles/2019/0-NAR.html\n",
       "1  2019   https://doi.org/10.1093/nar/gky993  articles/2019/1-NAR.html\n",
       "2  2019  https://doi.org/10.1093/nar/gky1124  articles/2019/2-NAR.html\n",
       "3  2019  https://doi.org/10.1093/nar/gky1069  articles/2019/3-NAR.html\n",
       "4  2019   https://doi.org/10.1093/nar/gky843  articles/2019/4-NAR.html"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_df.to_csv('article-path-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
