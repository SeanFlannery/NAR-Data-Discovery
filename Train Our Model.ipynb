{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Our Model\n",
    "Author: Sean Flannery [sflanner@purdue.edu](sflanner@purdue.edu)\n",
    "\n",
    "Last Updated: June 13th, 2019\n",
    "\n",
    "This notebook was developed with the intent of satisfying modeling needs of work with \n",
    "Professor Daisuke Kihara [dkihara@purdue.edu](dkihara@purdue.edu).\n",
    "### Description\n",
    "This notebook trains an instance of the Natural Language Processing model Doc2Vec.\n",
    "\n",
    "**Libraries Needed:** \n",
    "[pandas](https://pandas.pydata.org/pandas-docs/stable/install.html), \n",
    "[numpy](https://www.numpy.org), \n",
    "[gensim](https://radimrehurek.com/gensim/install.html), \n",
    "[tqdm](https://github.com/tqdm/tqdm), \n",
    "[scikit-learn](https://scikit-learn.org/stable/install.html),\n",
    "[nltk](https://www.nltk.org)\n",
    "\n",
    "**Reference:** This notebook relied heavily on information from a tutorial found here: [https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PREFIX = '' # optional! include to save files to certain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import random\n",
    "random.seed(42)\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# Progress bar\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# Document similarity\n",
    "import gensim\n",
    "import gensim.parsing.preprocessing as gpp\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedLineDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article-link</th>\n",
       "      <th>local-path</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>introduction</th>\n",
       "      <th>preprocessed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky993</td>\n",
       "      <td>articles/2019/1-NAR.html</td>\n",
       "      <td>Database Resources of the BIG Data Center in 2019</td>\n",
       "      <td>The BIG Data Center at Beijing Institute of Ge...</td>\n",
       "      <td>['BIG Data Center Members']</td>\n",
       "      <td>The BIG Data Center (http://bigd.big.ac.cn) at...</td>\n",
       "      <td>big center beij institute genomic big chinese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1124</td>\n",
       "      <td>articles/2019/2-NAR.html</td>\n",
       "      <td>The European Bioinformatics Institute in 2018:...</td>\n",
       "      <td>The European Bioinformatics Institute (https:/...</td>\n",
       "      <td>['Charles E Cook', 'Rodrigo Lopez', 'Oana Stro...</td>\n",
       "      <td>A primary mission of EMBL-EBI is to collect, o...</td>\n",
       "      <td>european bioinformatic institute ebi archive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1069</td>\n",
       "      <td>articles/2019/3-NAR.html</td>\n",
       "      <td>Database resources of the National Center for ...</td>\n",
       "      <td>The National Center for Biotechnology Informat...</td>\n",
       "      <td>['Eric W Sayers', 'Richa Agarwala', 'Evan E Bo...</td>\n",
       "      <td>The National Center for Biotechnology Informat...</td>\n",
       "      <td>national center biotechnology ncbi large suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky843</td>\n",
       "      <td>articles/2019/4-NAR.html</td>\n",
       "      <td>AmtDB: a database of ancient human mitochondri...</td>\n",
       "      <td>Ancient mitochondrial DNA is used for tracing ...</td>\n",
       "      <td>['Edvard Ehler', 'Jiří Novotný', 'Anna Juras',...</td>\n",
       "      <td>Ancient DNA (aDNA) is a genetic material obtai...</td>\n",
       "      <td>ancient mitochondrial dna trace human past de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky822</td>\n",
       "      <td>articles/2019/5-NAR.html</td>\n",
       "      <td>AnimalTFDB 3.0: a comprehensive resource for a...</td>\n",
       "      <td>The Animal Transcription Factor DataBase (Anim...</td>\n",
       "      <td>['Hui Hu', 'Ya-Ru Miao', 'Long-Hao Jia', 'Qing...</td>\n",
       "      <td>Transcription factors (TFs) are special protei...</td>\n",
       "      <td>animal transcription factor animaltfdb aim co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                         article-link                local-path  \\\n",
       "0  2019   https://doi.org/10.1093/nar/gky993  articles/2019/1-NAR.html   \n",
       "1  2019  https://doi.org/10.1093/nar/gky1124  articles/2019/2-NAR.html   \n",
       "2  2019  https://doi.org/10.1093/nar/gky1069  articles/2019/3-NAR.html   \n",
       "3  2019   https://doi.org/10.1093/nar/gky843  articles/2019/4-NAR.html   \n",
       "4  2019   https://doi.org/10.1093/nar/gky822  articles/2019/5-NAR.html   \n",
       "\n",
       "                                               title  \\\n",
       "0  Database Resources of the BIG Data Center in 2019   \n",
       "1  The European Bioinformatics Institute in 2018:...   \n",
       "2  Database resources of the National Center for ...   \n",
       "3  AmtDB: a database of ancient human mitochondri...   \n",
       "4  AnimalTFDB 3.0: a comprehensive resource for a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The BIG Data Center at Beijing Institute of Ge...   \n",
       "1  The European Bioinformatics Institute (https:/...   \n",
       "2  The National Center for Biotechnology Informat...   \n",
       "3  Ancient mitochondrial DNA is used for tracing ...   \n",
       "4  The Animal Transcription Factor DataBase (Anim...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                        ['BIG Data Center Members']   \n",
       "1  ['Charles E Cook', 'Rodrigo Lopez', 'Oana Stro...   \n",
       "2  ['Eric W Sayers', 'Richa Agarwala', 'Evan E Bo...   \n",
       "3  ['Edvard Ehler', 'Jiří Novotný', 'Anna Juras',...   \n",
       "4  ['Hui Hu', 'Ya-Ru Miao', 'Long-Hao Jia', 'Qing...   \n",
       "\n",
       "                                        introduction  \\\n",
       "0  The BIG Data Center (http://bigd.big.ac.cn) at...   \n",
       "1  A primary mission of EMBL-EBI is to collect, o...   \n",
       "2  The National Center for Biotechnology Informat...   \n",
       "3  Ancient DNA (aDNA) is a genetic material obtai...   \n",
       "4  Transcription factors (TFs) are special protei...   \n",
       "\n",
       "                                   preprocessed_data  \n",
       "0   big center beij institute genomic big chinese...  \n",
       "1   european bioinformatic institute ebi archive ...  \n",
       "2   national center biotechnology ncbi large suit...  \n",
       "3   ancient mitochondrial dna trace human past de...  \n",
       "4   animal transcription factor animaltfdb aim co...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv('preprocessed-data.csv')\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Vocabulary\n",
    "The vocabulary is little more than a glorified dictionary (held inside of `model.wv.vocab`), and contains all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word penalty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [gensim.models.doc2vec.TaggedDocument(doc.split(),[i])\n",
    "            for i, doc in enumerate(list(db['preprocessed_data']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['european', 'bioinformatic', 'institute', 'ebi', 'archive', 'curate', 'analyse', 'life', 'science', 'produce', 'researcher', 'world', 'make', 'use', 'globally', 'ebi', 'volume', 'continue', 'grow', 'exponentially', 'total', 'raw', 'storage', 'capacity', 'exceed', 'petabyte', 'manage', 'increase', 'flow', 'maintain', 'quality', 'service', 'year', 'improve', 'efficiency', 'computational', 'infrastructure', 'double', 'bandwidth', 'connection', 'worldwide', 'report', 'single', 'cell', 'expression', 'atla', 'ebi', 'gxa', 'component', 'expression', 'atla', 'pdbe', 'knowledgebase', 'ebi', 'pdbe', 'pdbe', 'collate', 'functional', 'annotation', 'prediction', 'structure', 'bank', 'additionally', 'europe', 'pmc', 'europepmc', 'org', 'add', 'preprint', 'abstract', 'result', 'supplement', 'result', 'peer', 'review', 'publication', 'embl', 'ebi', 'maintain', 'analytical', 'bioinformatic', 'complement', 'interface', 'programmatically', 'application', 'programme', 'interface', 'whilst', 'ensure', 'latest', 'version', 'train', 'team', 'support', 'staff', 'continue', 'site', 'off', 'site', 'train', 'opportunity', 'thousand', 'researcher', 'worldwide', 'year', 'primary', 'mission', 'embl', 'ebi', 'collect', 'organize', 'add', 'value', 'biomolecular', 'science', 'global', 'life', 'science', 'community', 'fulfil', 'mission', 'support', 'ebi', 'service', 'archival', 'store', 'primary', 'submit', 'researcher', 'knowledgebase', 'add', 'value', 'archival', 'curation', 'interface', 'accessible', 'application', 'programme', 'interface', 'api', 'high', 'throughput', 'method', 'access', 'additionally', 'exces', 'interface', 'api', 'allow', 'analyse', 'download', 'embl', 'ebi', 'public', 'repository', 'infrastructure', 'greater', 'below', 'fundamental', 'tenet', 'mission', 'host', 'infrastructure', 'freely', 'worldwide', 'represent', 'share', 'variety', 'structure', 'standard', 'format', 'consumption', 'people', 'machine', 'embl', 'ebi', 'worldwide', 'infrastructure', 'life', 'science', 'run', 'collaboration', 'partner', 'world', 'describe', 'year', 'continue', 'international', 'collaboration', 'crucial', 'ensure', 'life', 'science', 'infrastructure', 'open', 'access', 'archive', 'add', 'value', 'ever', 'grow', 'volume', 'submit', 'researcher', 'globe', 'embl', 'ebi', 'node', 'elixir', 'infrastructure', 'elixir', 'europe', 'org', 'europe', 'actively', 'engage', 'effort', 'increase', 'efficiency', 'global', 'infrastructure', 'life', 'science', 'continuously', 'review', 'service', 'consultation', 'monitor', 'technology', 'advance', 'research', 'order', 'ensure', 'service', 'need', 'undertake', 'work', 'update', 'summarize', 'growth', 'service', 'introduce', 'year', 'overview', 'extensive', 'broad', 'collection', 'complement', 'discovery', 'ontology', 'transcend', 'scientific', 'boundary', 'bring', 'biologist', 'clinician', 'physicist', 'mathematician', 'software', 'engineer', 'academia', 'industry', 'healthcare', 'connect', 'diverse', 'skill', 'expertise', 'enable', 'creative', 'exploration', 'complex', 'question', 'biology', 'today'], [1])\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Doc2Vec Instance\n",
    "\n",
    "The basis for these parameters came from this paper: https://www.aclweb.org/anthology/W16-1609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with `documents` took 20.971 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model = gensim.models.doc2vec.Doc2Vec(\n",
    "                documents=documents,\n",
    "                vector_size=300,\n",
    "                alpha=0.002, \n",
    "                dm = 1, # we want to use distributed memory not just Bag of Words\n",
    "                dm_concat = 1, # Use more context (generates larger model)\n",
    "                min_alpha=0.0001, \n",
    "                min_count=1,\n",
    "                epochs=1000,\n",
    "                negative=5,\n",
    "                window=5, # have also used 15\n",
    "                sample=0.00001,\n",
    "                seed=42,\n",
    "                max_vocab_size=None, # No limit on the size of our built vocabulary...\n",
    "                workers=8,\n",
    "                train_lbls=False)\n",
    "file_time = time.time() - start_time\n",
    "print(\"Training model with `documents` took {:.3f} minutes\".format(file_time/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model.save(FILE_PREFIX + \"trained.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment if you would like to load a local model\n",
    "#model = gensim.models.Doc2Vec.load(FILE_PREFIX + \"trained.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Self-Similarity\n",
    "We are going to see how well our model can infer itself and record how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vector(doc_id):\n",
    "    return model.infer_vector(train_list[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a dictionary of similarities to catalogue the `i-th` most similar entries to each document vector. This will help us determine if our model is robust enough to predict itself as being most similar when given its own words as input.\n",
    "\n",
    "We define `getRanks` to fetch the most similar documents based on the model's document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c357bafa0fa745b8986be32a42a857cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def getRanks(doc_id):\n",
    "    res_dict = {'doc_id':doc_id}\n",
    "    inferred_vector = model.infer_vector(documents[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    res_dict['rank'] = [docid for docid, sim in sims].index(doc_id)\n",
    "    res_dict['second_rank'] = sims[1]\n",
    "    res_dict['sims'] = sims\n",
    "    return res_dict\n",
    "entry_range = list(range(len(documents)))\n",
    "#with Pool(25) as p:\n",
    "with Pool(8) as p:\n",
    "    req_list = list(tqdm(p.imap(getRanks, entry_range), total=len(entry_range), leave=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "sim_dict = dict()\n",
    "for ent in req_list:\n",
    "    ranks.append(ent['rank'])\n",
    "    sim_dict[ent['doc_id']] = ent['sims']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would hope to have a high number of entries in the 0 range (indicating most-similar document is the original document itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3111), (1, 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of articles are able to be ranked to themselves. Let's see what other conclusions we may draw from a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (2619)\n",
      "\n",
      "trEST, trGEN and Hits: access to databases of predicted protein sequences\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d300,n5,w5,s1e-05,t8):\n",
      "\n",
      "(2619, 0.9403233528137207)\n",
      "MOST \n",
      "(2619, 0.9403233528137207) \n",
      "trEST, trGEN and Hits: access to databases of predicted protein sequences\n",
      "\n",
      "SECOND-MOST \n",
      "(2317, 0.4497912526130676) \n",
      "trome, trEST and trGEN: databases of predicted protein sequences\n",
      "\n",
      "THIRD-MOST \n",
      "(1933, 0.27196189761161804) \n",
      "ARED 3.0: the large and diverse AU-rich transcriptome\n",
      "\n",
      "FOURTH-MOST \n",
      "(1912, 0.24466779828071594) \n",
      "DDBJ in preparation for overview of research activities behind data submissions\n",
      "\n",
      "FIFTH-MOST \n",
      "(2357, 0.23794858157634735) \n",
      "TheArabidopsisSeedGenes Project\n",
      "\n",
      "SIXTH-MOST \n",
      "(2492, 0.23058755695819855) \n",
      "FLAGdb/FST: a database of mapped flanking insertion sites (FSTs) ofArabidopsis thalianaT-DNA transformants\n",
      "\n",
      "SEVENTH-MOST \n",
      "(2389, 0.22896608710289001) \n",
      "TheArabidopsisInformation Resource (TAIR): a model organism database providing a centralized, curated gateway toArabidopsisbiology, research materials and community\n",
      "\n",
      "EIGHT-MOST \n",
      "(2280, 0.222357377409935) \n",
      "PHYTOPROT: a database of clusters of plant proteins\n",
      "\n",
      "NINTH-MOST \n",
      "(2547, 0.22165623307228088) \n",
      "SYSTERS, GeneNest, SpliceNest: exploring sequence space from genome to protein\n",
      "\n",
      "TENTH-MOST \n",
      "(2128, 0.2165200412273407) \n",
      "The Diatom EST Database\n",
      "\n",
      "MEDIAN \n",
      "(2700, -0.0007361918687820435) \n",
      "YIDB: the Yeast Intron DataBase\n",
      "\n",
      "LEAST \n",
      "(2782, -0.21277953684329987) \n",
      "Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "doc_id = random.randint(0, 3114)\n",
    "print('Document ({})\\n'.format(doc_id))\n",
    "print(db.loc[doc_id, 'title'])\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "print(sim_dict[doc_id][0])\n",
    "for label, index in [\n",
    "    ('MOST', 0), \n",
    "    ('SECOND-MOST', 1), \n",
    "    ('THIRD-MOST', 2), \n",
    "    ('FOURTH-MOST', 3), \n",
    "    ('FIFTH-MOST', 4), \n",
    "    ('SIXTH-MOST', 5),\n",
    "    ('SEVENTH-MOST', 6), \n",
    "    ('EIGHT-MOST', 7),\n",
    "    ('NINTH-MOST', 8), \n",
    "    ('TENTH-MOST', 9),\n",
    "    ('MEDIAN', len(sim_dict[0])//2), \n",
    "    ('LEAST', len(sim_dict[0]) - 1)]:\n",
    "    print(u'%s \\n%s \\n%s\\n' % (label, sim_dict[doc_id][index], str(db.loc[sim_dict[doc_id][index][0], 'title'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Similarities\n",
    "Now, we will randomly pick an example and see what the second most similar document might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (2069): «The TIGR Gene Indices: clustering and assembling EST and known genes and integration with eukaryotic genomes»\n",
      "\n",
      "Similar Document (2622): «The TIGR Gene Indices: analysis of gene transcript sequences in highly sampled eukaryotic species»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(documents) - 1)\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, db.loc[doc_id, 'title']))\n",
    "sim_id = sim_dict[doc_id][1][0]\n",
    "print('Similar Document ({}): «{}»\\n'.format(sim_id, db.loc[sim_id, 'title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Similarity Confidence Levels\n",
    "Now, we want to systematically evaluate our model for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Correct Predictions for self-similarity: 99.936%\n",
      "Average Confidence of self-similarity correctness: 95.538%\n"
     ]
    }
   ],
   "source": [
    "number_correct = 0.\n",
    "confidence_sum = 0.\n",
    "for original_id in range(len(documents)):\n",
    "    if sim_dict[original_id][0][0] == original_id:\n",
    "        number_correct += 1\n",
    "        confidence_sum += sim_dict[original_id][0][1]\n",
    "print(\"Percentage Correct Predictions for self-similarity: %.3f%%\" % (100.* number_correct / len(documents)))\n",
    "print(\"Average Confidence of self-similarity correctness: %.3f%%\" % (100.* confidence_sum / number_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i-th Similarity Confidence Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_range = [0,1,2,3,4,5,10,15,20,25,100,250,500,1000,2000,3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted similarity % for rank 0: 95.537%\n",
      "Predicted similarity % for rank 1: 52.687%\n",
      "Predicted similarity % for rank 2: 45.244%\n",
      "Predicted similarity % for rank 3: 40.751%\n",
      "Predicted similarity % for rank 4: 37.638%\n",
      "Predicted similarity % for rank 5: 35.581%\n",
      "Predicted similarity % for rank 10: 29.951%\n",
      "Predicted similarity % for rank 15: 27.133%\n",
      "Predicted similarity % for rank 20: 25.387%\n",
      "Predicted similarity % for rank 25: 24.114%\n",
      "Predicted similarity % for rank 100: 17.003%\n",
      "Predicted similarity % for rank 250: 12.366%\n",
      "Predicted similarity % for rank 500: 8.473%\n",
      "Predicted similarity % for rank 1000: 3.769%\n",
      "Predicted similarity % for rank 2000: -3.262%\n",
      "Predicted similarity % for rank 3000: -14.692%\n"
     ]
    }
   ],
   "source": [
    "for i in sim_range:\n",
    "    conf_sum = 0.\n",
    "    total_count = 0.\n",
    "    for original_id in range(len(documents)):\n",
    "        conf_sum += sim_dict[original_id][i][1]\n",
    "        total_count += 1\n",
    "    print(\"Predicted similarity % for rank \" + str(i) + \": %.3f%%\" % (100. * conf_sum / total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the resultant embeddings from our documents based on the `docvecs` result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_range = list(range(len(documents)))\n",
    "def getVectorFromCorpus(doc_id):\n",
    "    return model.docvecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87593f2d2b74449bad98f274356291aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Pool(25) as p:\n",
    "    req_list = list(tqdm(p.imap(getVectorFromCorpus, entry_range), total=len(entry_range), leave=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.float64(req_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(FILE_PREFIX + 'doc_embeddings.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
