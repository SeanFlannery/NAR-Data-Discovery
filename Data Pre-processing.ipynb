{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "Author: Sean Flannery [sflanner@purdue.edu](sflanner@purdue.edu)\n",
    "\n",
    "Last Updated: June 13th, 2019\n",
    "\n",
    "This notebook was developed with the intent of satisfying modeling needs of work with \n",
    "Professor Daisuke Kihara [dkihara@purdue.edu](dkihara@purdue.edu).\n",
    "### Description\n",
    "This notebook displays how to preprocess data for eventual use with Natural Language Processing models like Doc2Vec.\n",
    "\n",
    "**Libraries Needed:** \n",
    "[pandas](https://pandas.pydata.org/pandas-docs/stable/install.html), \n",
    "[numpy](https://www.numpy.org), \n",
    "[gensim](https://radimrehurek.com/gensim/install.html), \n",
    "[scikit-learn](https://scikit-learn.org/stable/install.html),\n",
    "[pattern](https://github.com/clips/pattern),\n",
    "[nltk](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import random\n",
    "random.seed(42)\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# Document similarity\n",
    "import gensim\n",
    "import gensim.parsing.preprocessing as gpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specification \n",
    "We want to preprocess the text data in `nar-complete-data.{csv,json}` so that we may generate useful embeddings in a later module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article-link</th>\n",
       "      <th>local-path</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>introduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky993</td>\n",
       "      <td>articles/2019/1-NAR.html</td>\n",
       "      <td>Database Resources of the BIG Data Center in 2019</td>\n",
       "      <td>The BIG Data Center at Beijing Institute of Ge...</td>\n",
       "      <td>['BIG Data Center Members']</td>\n",
       "      <td>The BIG Data Center (http://bigd.big.ac.cn) at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1124</td>\n",
       "      <td>articles/2019/2-NAR.html</td>\n",
       "      <td>The European Bioinformatics Institute in 2018:...</td>\n",
       "      <td>The European Bioinformatics Institute (https:/...</td>\n",
       "      <td>['Charles E Cook', 'Rodrigo Lopez', 'Oana Stro...</td>\n",
       "      <td>A primary mission of EMBL-EBI is to collect, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky1069</td>\n",
       "      <td>articles/2019/3-NAR.html</td>\n",
       "      <td>Database resources of the National Center for ...</td>\n",
       "      <td>The National Center for Biotechnology Informat...</td>\n",
       "      <td>['Eric W Sayers', 'Richa Agarwala', 'Evan E Bo...</td>\n",
       "      <td>The National Center for Biotechnology Informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky843</td>\n",
       "      <td>articles/2019/4-NAR.html</td>\n",
       "      <td>AmtDB: a database of ancient human mitochondri...</td>\n",
       "      <td>Ancient mitochondrial DNA is used for tracing ...</td>\n",
       "      <td>['Edvard Ehler', 'Jiří Novotný', 'Anna Juras',...</td>\n",
       "      <td>Ancient DNA (aDNA) is a genetic material obtai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1093/nar/gky822</td>\n",
       "      <td>articles/2019/5-NAR.html</td>\n",
       "      <td>AnimalTFDB 3.0: a comprehensive resource for a...</td>\n",
       "      <td>The Animal Transcription Factor DataBase (Anim...</td>\n",
       "      <td>['Hui Hu', 'Ya-Ru Miao', 'Long-Hao Jia', 'Qing...</td>\n",
       "      <td>Transcription factors (TFs) are special protei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                         article-link                local-path  \\\n",
       "0  2019   https://doi.org/10.1093/nar/gky993  articles/2019/1-NAR.html   \n",
       "1  2019  https://doi.org/10.1093/nar/gky1124  articles/2019/2-NAR.html   \n",
       "2  2019  https://doi.org/10.1093/nar/gky1069  articles/2019/3-NAR.html   \n",
       "3  2019   https://doi.org/10.1093/nar/gky843  articles/2019/4-NAR.html   \n",
       "4  2019   https://doi.org/10.1093/nar/gky822  articles/2019/5-NAR.html   \n",
       "\n",
       "                                               title  \\\n",
       "0  Database Resources of the BIG Data Center in 2019   \n",
       "1  The European Bioinformatics Institute in 2018:...   \n",
       "2  Database resources of the National Center for ...   \n",
       "3  AmtDB: a database of ancient human mitochondri...   \n",
       "4  AnimalTFDB 3.0: a comprehensive resource for a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The BIG Data Center at Beijing Institute of Ge...   \n",
       "1  The European Bioinformatics Institute (https:/...   \n",
       "2  The National Center for Biotechnology Informat...   \n",
       "3  Ancient mitochondrial DNA is used for tracing ...   \n",
       "4  The Animal Transcription Factor DataBase (Anim...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                        ['BIG Data Center Members']   \n",
       "1  ['Charles E Cook', 'Rodrigo Lopez', 'Oana Stro...   \n",
       "2  ['Eric W Sayers', 'Richa Agarwala', 'Evan E Bo...   \n",
       "3  ['Edvard Ehler', 'Jiří Novotný', 'Anna Juras',...   \n",
       "4  ['Hui Hu', 'Ya-Ru Miao', 'Long-Hao Jia', 'Qing...   \n",
       "\n",
       "                                        introduction  \n",
       "0  The BIG Data Center (http://bigd.big.ac.cn) at...  \n",
       "1  A primary mission of EMBL-EBI is to collect, o...  \n",
       "2  The National Center for Biotechnology Informat...  \n",
       "3  Ancient DNA (aDNA) is a genetic material obtai...  \n",
       "4  Transcription factors (TFs) are special protei...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv('complete-article-data.csv')\n",
    "db = db.reset_index(drop=True)\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab our text\n",
    "We need to define a function `read_corpus_intro`, which yields simply pre-processed results over all introductions we've read in.\n",
    "\n",
    "We want to remove stopwords, numbers, puncutation etc. Here we use the preprocessing filters described on the gensim documents: [https://radimrehurek.com/gensim/parsing/preprocessing.html](https://radimrehurek.com/gensim/parsing/preprocessing.html)\n",
    "\n",
    "We will also use the Natural Language Tool Kit (NLTK) and pattern's \n",
    "powerful [lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sean/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from pattern.en import lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Preprocessing Function: `basic_preprocess`\n",
    "- Takes as input a string representing a document\n",
    "- [`remove_stopwords`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.remove_stopwords): Remove [stopwords](https://en.wikipedia.org/wiki/Stop_words) from given String\n",
    "- [`strip_non_alphanum`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_non_alphanum): Remove non-alphabetic characters\n",
    "- [`strip_punctuation*`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_punctuation): Unicode string without punctuation characters\n",
    "- [`strip_tags`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_tags): Remove any remaining html tags\n",
    "- [`strip_numeric`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_numeric): Remove digits\n",
    "- [`strip_multiple_whitespaces`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_multiple_whitespaces): Remove repeating whitespace characters (spaces, tabs, line breaks) and turns tabs & line breaks into spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess(string):\n",
    "    string_list = gpp.preprocess_string(string.lower(), \n",
    "        filters=[\n",
    "            gpp.remove_stopwords, \n",
    "            gpp.strip_non_alphanum, \n",
    "            gpp.strip_punctuation, gpp.strip_punctuation2,\n",
    "            gpp.strip_tags, \n",
    "            gpp.strip_numeric,\n",
    "            gpp.strip_multiple_whitespaces])\n",
    "    return [ent.lower().strip()\n",
    "            for ent in string_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'returned', 'use', 'basic', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "print(basic_preprocess('''\n",
    "                This is an example of what will be returned\n",
    "                if we use just basic_preprocessing.\n",
    "                '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) Function: `lemmatize_strings`:\n",
    "- This function takes as input a list of strings\n",
    "- Reduces inflectional forms of words to a base form\n",
    "- Returns list of lemmatized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_strings(string_list):\n",
    "    return [lemma(ent.lower().strip()) \n",
    "                for ent in string_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This handles a small issue with generators during runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lemmatize_strings('test me'.split())\n",
    "except StopIteration and RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'be', 'an', 'example', 'of', 'what', 'will', 'be', 'return', 'if', 'we', 'use', 'just', 'lemmatization.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_strings('''\n",
    "                This is an example of what will be returned\n",
    "                if we use just lemmatization.\n",
    "                '''.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'return', 'use', 'lemmatization', 'use', 'basic', 'preprocess', 'function']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_strings(\n",
    "        basic_preprocess(\n",
    "                '''\n",
    "                This is an example of what will be returned\n",
    "                if we use lemmatization after we \n",
    "                use our basic_preprocessing function.\n",
    "                ''')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Corpus from Pandas DB: `read_corpus`:\n",
    "- This function takes as input a list of strings\n",
    "- `features_desired`: optional input of what DataFrame columns to take\n",
    "- `min_size`: optional input for the minimum-sized string to accept AFTER preprocessing and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(pandas_db, features_desired=['introduction'], min_size=0):\n",
    "    for index, row in pandas_db.iterrows():\n",
    "        # Create the desired feature string\n",
    "        rowStr = ''.join([str(row[feature]) + ' '\n",
    "                          for feature in features_desired])\n",
    "        rowStrList = lemmatize_strings(basic_preprocess(rowStr))\n",
    "        res = [ent for ent in rowStrList if len(ent) >= min_size]\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate a list of strings that we will eventually train our models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_corpus = list(read_corpus(\n",
    "                    db, \n",
    "                    features_desired=['abstract', 'introduction'],\n",
    "                    min_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Computation\n",
    "\n",
    "\n",
    "\n",
    "You will defeat the whole purpose of IDF weighting if its not based on a large corpora as (a) your vocabulary becomes too small and (b) you have limited ability to observe the behavior of words that you do know about.\n",
    "\n",
    "Below code sourced from [here](http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XQBj3y3MxQI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_corpus = []\n",
    "for ent in pp_corpus:\n",
    "    tmp = \" \"\n",
    "    for word in ent:\n",
    "        tmp += word + ' '\n",
    "    docs_corpus.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    # Maximum % of documents where the word may exist\n",
    "    max_df=0.50, # remove all words that are in >=50% of documents\n",
    "    # Extra stopword filter provided by sklearn\n",
    "    stop_words='english',\n",
    "    smooth_idf=True,\n",
    "    use_idf=True,\n",
    "    strip_accents='unicode')\n",
    "word_count_vector = tfidf.fit_transform(docs_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend reviewing the below set of words to see if it is acceptable for them to be excluded for you. You can adjust the `max_df` value above to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study',\n",
       " 'contain',\n",
       " 'genome',\n",
       " 'include',\n",
       " 'database',\n",
       " 'http',\n",
       " 'data',\n",
       " 'user',\n",
       " 'provide',\n",
       " 'protein',\n",
       " 'tool',\n",
       " 'search',\n",
       " 'base',\n",
       " 'new',\n",
       " 'information',\n",
       " 'analysi',\n",
       " 'sequence',\n",
       " 'develop',\n",
       " 'available',\n",
       " 'web',\n",
       " 'number',\n",
       " 'www',\n",
       " 'resource',\n",
       " 'gene']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf.stop_words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the new stopwords from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = set(tfidf.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_res = []\n",
    "for pp_list in pp_corpus:\n",
    "    tmp = []\n",
    "    for word in pp_list:\n",
    "        if word not in new_stopwords:\n",
    "            tmp.append(word)\n",
    "    tmp_res.append(tmp)\n",
    "pp_corpus = tmp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering High-Count Words\n",
    "You also have the option of removing high-count words that were not captured in the original TF-IDF filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(30329 unique tokens: ['academia', 'academy', 'access', 'accessible', 'activity']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(pp_corpus)\n",
    "print(dictionary)\n",
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert this to a gensim [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)-style corpus for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in pp_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might be interested in creating a massive list of all the strings in our corpus, and then quantify this using a python Counter to view the top most common entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = collections.Counter([inner\n",
    "    for outer in pp_corpus\n",
    "        for inner in outer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the the top 10 most common words now, and if you see fit, you may remove them. We chose to retain them for this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('structure', 6836), ('human', 5582), ('annotation', 5500), ('site', 4352), ('interaction', 4085), ('domain', 4035), ('rna', 3982), ('function', 3962), ('model', 3772), ('genomic', 3706)]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "counter_result = ctr.most_common(n)\n",
    "print(counter_result)\n",
    "topn = [word[0] for word in counter_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_corpus = []\n",
    "for ent in pp_corpus:\n",
    "    tmp = \" \"\n",
    "    for word in ent:\n",
    "        tmp += word + ' '\n",
    "    docs_corpus.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['preprocessed_data'] = docs_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.to_csv('preprocessed-data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
